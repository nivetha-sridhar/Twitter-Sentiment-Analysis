import pandas as pd
import numpy as np
import re
import string

#preproccessing
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
from nltk.stem import WordNetLemmatizer
from nltk import FreqDist
from sklearn.model_selection import train_test_split 

#Visualization
import seaborn as sns
import matplotlib.pyplot as plt
from wordcloud import WordCloud

#feature extraction
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer

#machine learning libraries
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import metrics

stop_words = set(stopwords.words("english"))

import warnings
warnings.filterwarnings("ignore")

df = pd.read_excel("/kaggle/input/twitter-dataset/Tweets1.xlsx")

df.info()
df.describe()
df.shape
df.isnull().sum()
df = df.dropna()

#Text Preprocessing

Z = df["selected_text"].astype("string")

def preprocess_tweets(tweet):

    #converting all tweest to lowercase
    tweet =  tweet.lower()

    #removing urls using regx
    tweet = re.sub(r"http\S+|www\S+|https\S+","",tweet , flags = re.MULTILINE) #MULTILINE flag checks for the regx at the start of the string or even inside the string
      
    #remove punctuations
    tweet =  tweet.translate(str.maketrans("","",string.punctuation))

    #remove # and @
    tweet = re.sub(r"\@\w+|\#" , "",tweet)

    #remove stopwords
    tweet_tokens = word_tokenize(tweet)
    filtered_words = [word for word in tweet_tokens if word not in stop_words]
    
    # stemming
    ps = PorterStemmer()
    stem_words = [ps.stem(words) for words in filtered_words]

    #lemmatization
    lemmatizer = WordNetLemmatizer()
    lemma_words = [lemmatizer.lemmatize(words , pos = "a") for words in stem_words]

    return " ".join(lemma_words)

df["text_preprocessed"] = Z.apply(preprocess_tweets)

# df["text_preprocessed"]

# df["sentiment"]= df["sentiment"].apply(lambda x:1 if x=="positive"else x)
# df["sentiment"]= df["sentiment"].apply(lambda x:0 if x=="negative"else x)
# df["sentiment"]= df["sentiment"].apply(lambda x:2 if x=="neutral"else x)

df["sentiment"].value_counts()
